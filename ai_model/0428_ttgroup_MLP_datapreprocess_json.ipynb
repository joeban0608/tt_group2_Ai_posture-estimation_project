{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0428_ttgroup_MLP_datapreprocess_json.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6nLePyDglVM"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahnuW8rfKSkZ"
      },
      "source": [
        "#記得掛載雲端硬碟 \n",
        "from zipfile import ZipFile\n",
        "path = \"/content/drive/MyDrive/ttgroup/0_9000/0519_out_resize256x256_0_9000_json.zip\"\n",
        "f = ZipFile(path)\n",
        "# f.extractall() 小括號是直接解壓縮在同一層\n",
        "f.extractall()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNoY0fsKOqw3"
      },
      "source": [
        "# img_fn_df 圖片名稱DataFrame\n",
        "import glob\n",
        "import json, codecs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "fn_dic = {\"img_name\":[]}\n",
        "# fn_dic_2 = []\n",
        "paths = sorted(glob.glob(\"0519_out_resize256x256_0_9000_json/*\"))\n",
        "for path in paths:\n",
        "  fn_type = path.split(\"/\")[-1]\n",
        "  fn = fn_type.split(\".\")[0]\n",
        "  fn_dic[\"img_name\"].append(fn)\n",
        "  # fn_dic_2.append(fn)\n",
        "# print(fn_dic)\n",
        "fn_dic_df = pd.DataFrame(fn_dic)\n",
        "fn_dic_df\n",
        "# print(fn_dic_2)\n",
        "\n",
        "\n",
        "# file_path = \"/content/1162.json\"\n",
        "# for i in range(len(fs))\n",
        "# obj_text = codecs.open(paths[0], 'r', encoding='utf-8').read()\n",
        "# b_new = json.loads(obj_text)\n",
        "# a_new = np.array(b_new)\n",
        "# a_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDtC4J5xzKMG"
      },
      "source": [
        "# ans_df  驗證資料DataFrame\n",
        "import os\n",
        "base = \"/content/drive/MyDrive/ttgroup/0_9000/0519_wrong_total_sit_value_combinefoot_0_9000.csv\"\n",
        "sitting_csv = pd.read_csv(base, sep=\",\")\n",
        "ans_df = pd.DataFrame(sitting_csv)\n",
        "ans_df = ans_df.drop([\"img_name\"], axis=1)\n",
        "ans_df\n",
        "# ans = df[:1]\n",
        "# ans = ans.drop([\"img_name\"], axis=1)\n",
        "# ans\n",
        "# print(type(sitting_csv)\n",
        "# np.array(sitting_csv)\n",
        "# ans_head_df = pd.DataFrame(ans_df[\"head\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoH2dZ0UjuYe"
      },
      "source": [
        "# keypoints_36_combine_df 關節節點DataFrame\n",
        "import json, codecs\n",
        "import numpy as np\n",
        "columns ={\n",
        "     \"coco_col\":['Nose_x', 'Nose_y', 'Neck_x', 'Neck_y',\n",
        "           'R-Sho_x', 'R-Sho_y', 'R-Elb_x', 'R-Elb_y',\n",
        "           'R-Wr_x', 'R-Wr_y', 'L-Sho_x', 'L-Sho_y', \n",
        "           'L-Elb_x', 'L-Elb_y', 'L-Wr_x', 'L-Wr_y',\n",
        "           'R-Hip_x', 'R-Hip_y', 'R-Knee_x', 'R-Knee_y',\n",
        "           'R-Ank_x', 'R-Ank_y', 'L-Hip_x', 'L-Hip_y',\n",
        "           'L-Knee_x', 'L-Knee_y', 'L-Ank_x', 'L-Ank_y',\n",
        "           'R-Eye_x', 'R-Eye_y', 'L-Eye_x', 'L-Eye_y',\n",
        "           'R-Ear_x', 'R-Ear_y', 'L-Ear_x', 'L-Ear_y']\n",
        "    }\n",
        "fn_list = fn_dic[\"img_name\"]\n",
        "# print(fn_list) 印0004, 0006...\n",
        "\n",
        "# 運用globals()[] 全域變數：做成動態的變數名。\n",
        "n_fn_list = []\n",
        "for fn in fn_list:\n",
        "  # print(fn)\n",
        "  obj_text = codecs.open(\"/content/0519_out_resize256x256_0_9000_json/\"+ fn + \".json\", 'r', encoding='utf-8').read()\n",
        "  b_new = json.loads(obj_text)\n",
        "  a_new = np.array(b_new)\n",
        "  a_new = a_new.reshape(1,36)\n",
        "  # print(a_new)\n",
        "  globals()[\"key_point_36\"+ fn +\"_df\"] = pd.DataFrame(a_new, columns = columns[\"coco_col\"])\n",
        "  # print(globals()[\"key_point_36\"+ fn +\"_df\"])\n",
        "  n_fn_list.append(globals()[\"key_point_36\"+ fn +\"_df\"])\n",
        "# print(n_fn_list)\n",
        "\n",
        "# pd.concat([key_point_36_0004_df, key_point_36_0006_df, key_point_36_0008_df])\n",
        "key_point_36_combine_df = pd.concat(n_fn_list)\n",
        "key_point_36_combine_df\n",
        "\n",
        "# bond.reset_index(inplace=True)\n",
        "key_point_36_combine_df = key_point_36_combine_df.reset_index()\n",
        "key_point_36_combine_df\n",
        "\n",
        "# df.drop(columns=['B', 'C'])\n",
        "# df.drop(['B', 'C'], axis=1)\n",
        "key_point_36_combine_df = key_point_36_combine_df.drop(columns=['index'])\n",
        "key_point_36_combine_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb80G03d9LV-"
      },
      "source": [
        "# key_point_36_combine_df, class_name_df\n",
        "pd.concat([key_point_36_combine_df, ans_df], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGqtCEHfgO-g"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JKNPXAbkOse"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(np.array(key_point_36_combine_df),\n",
        "                             np.array(ans_df),\n",
        "                             test_size=0.1)\n",
        "# x_train, x_test, y_train, y_test  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA70hxP3lYS_"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzcZdkudkGsx"
      },
      "source": [
        "#　調整Dense 第一個參數為256\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# 調整參數\n",
        "DENSE = 32\n",
        "layers = [\n",
        "          # 36 * 128 + 128 bias (對activation做前置偏移調整)\n",
        "          Dense(DENSE, activation = \"relu\", input_dim = 36),\n",
        "          # 128 * 3 + 3 bias (對activation做前置偏移調整)\n",
        "          Dense(3, activation = \"sigmoid\")\n",
        "]\n",
        "model = Sequential(layers)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK10K4yrktPk"
      },
      "source": [
        "#　設定Loss，用model.compile\n",
        "# 如果是一個神經元(要視為兩個機率來做): Binary(二元的) 用 BinaryCrossentropy\n",
        "# 多個神經元: sigma(pi*log(1/qi)): CategoricalCrossEntropy\n",
        "# 這邊有多個神經元．所以用CategoricalCrossEntropy\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "# model.compile(loss=\"mse\",\n",
        "# 這邊不用MES因為坡度變化太小，很難看到誤差，所以用CrossEntropy來設定目標 \n",
        "model.compile(loss=BinaryCrossentropy(),\n",
        "       metrics=[\"accuracy\"],\n",
        "       optimizer=\"adam\")\n",
        "# optimizer 優化器：動量優化，解決局部最小值的問題，像彈珠\n",
        "#           動態步長,前面走大步一點，後面越走越小步，快到目的地了。(像找門牌)\n",
        "# 兩個結合起來即\"adam\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXFU964PneJy"
      },
      "source": [
        "# # 用to_categorical 做one-hot-encodin, 將訓練跟測試的答案分別放到to_categorical 做分類(y_train),to_categorical(y_test)  \n",
        "# # 記得num_classes要帶入,這邊[0]~[9],共10種所以放入10, 做完之後用訓練跟測試的答案命名\n",
        "# # num_classes=10，強迫給出10個機率，以免資料錯誤\n",
        "# from tensorflow.keras.utils import to_categorical\n",
        "# # y_train_cat = to_categorical(y_train, num_classes=10)\n",
        "# # y_test_cat = to_categorical(y_test, num_classes=10)\n",
        "# y_train_cat = to_categorical(y_train, 10)\n",
        "# y_test_cat = to_categorical(y_test, 10)\n",
        "# print(y_train[0]) #改過前\n",
        "# print(y_train_cat[0]) #改過後　one-hot-encoding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA8sMgqAniRv"
      },
      "source": [
        "# # reshape 將訓練跟測試資料的題目改為一維,原本是(60000, 28, 28)→(-1, 784)\n",
        "# # 1-1-1\n",
        "# # x_train.reshape(60000, 784)\n",
        "# # x_test.reshape(10000, 784)\n",
        "# # 1-1-2\n",
        "# # # 可以利用reshape隱藏參數[-1]：60000，784其中一個也可寫成-1（代表全部數量）,請機器自己算。 #不能兩個參數都調-1\n",
        "# # x_train.reshape(-1, 784)\n",
        "# # x_test.reshape(-1, 784)\n",
        " \n",
        "# # 將散佈範圍調整至-1至1之間，tensorflow的input_range的淺規則是在(-1,1) or (0,1)　之間；\n",
        "# # 在以前caffe函式庫分析圖片時不用做轉換，因為本來input_range就是(0, 255)\n",
        "# x_train_shape = x_train.reshape(-1, 784) / 255.0\n",
        "# x_test_shape = x_test.reshape(-1, 784) / 255.0\n",
        "# x_train_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uvxob_JOnSya"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "callbacks = [\n",
        "   EarlyStopping(patience=5, restore_best_weights=True)\n",
        "]\n",
        "model.fit(x_train,\n",
        "     y_train,\n",
        "     batch_size=20,\n",
        "     epochs=50,\n",
        "     validation_split=0.1,\n",
        "     verbose=2,\n",
        "     callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBJpJJJlnW53"
      },
      "source": [
        "# 用model.evaluate評估模型 \n",
        "# evaluate完去調dense 128 -> 256/64/512去找到最佳, 最後256跟512差不多,那就用256, 用越少的神經元學習效率較高\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QycFlK4Y6EGi"
      },
      "source": [
        "Dense:128\n",
        "batch_size:15\n",
        "val_accuracy:0.56, 0.5, 0.43, 0.54, 0.47, 0.56, 0.69\n",
        "val_loss:1.6, 1.55, 2.23, 1.41 2.17, 1.23, 1.43, 2.08"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK8n8jnc2B4k"
      },
      "source": [
        "ans_names = ans_df.columns.tolist()\n",
        "# key_point_36_combine_columns_list\n",
        "key_point_36_combine_columns_list = key_point_36_combine_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ixXUrCkqEz_"
      },
      "source": [
        "# model.predict()\n",
        "# input \n",
        "# x_test[:][0].shape\n",
        "# x_test_r = x_test[:][0].reshape(1,36) \n",
        "# pre = model.predict(x_test_r)\n",
        "# pre_list = pre[0].tolist()\n",
        "# for i, ans_name in enumerate(ans_names):\n",
        "#   print(ans_name + \":\", round(pre_list[i]))\n",
        "  \n",
        "# x_test[:][0].shape\n",
        "# pre_df = pd.DataFrame(pre)\n",
        "# pre_df[0].apply(round)\n",
        "# for i in range(4):\n",
        "#   print(pre_df[i].apply(round))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLImgB-uZYY9"
      },
      "source": [
        "# input_test\n",
        "fn = input(\"請輸入檔名:\")\n",
        "obj_text = codecs.open(fn, 'r', encoding='utf-8').read()\n",
        "b_new = json.loads(obj_text)\n",
        "a_new = np.array(b_new)\n",
        "a_new = a_new.reshape(1,36)\n",
        "input_key_point_36_df = pd.DataFrame(a_new, columns=key_point_36_combine_columns_list)\n",
        "input_key_point_36_df\n",
        "\n",
        "# input_key_point_36_df.shape\n",
        "\n",
        "# 印出head, shoulder, foot 是 1 的機率\n",
        "x_test_in = input_key_point_36_df\n",
        "x_test_in\n",
        "pre = model.predict(x_test_in)[0]\n",
        "\n",
        "# 將機率清單轉成T&F\n",
        "ans_names = ans_df.columns.tolist()\n",
        "pre_list = pre.tolist()\n",
        "print(\"預測數值\", pre_list)\n",
        "\n",
        "ans_list = []\n",
        "ans_name_list = []\n",
        "for i, ans_name in enumerate(ans_names):\n",
        "  print(ans_name + \":\", round(pre_list[i]))\n",
        "  ans_list.append(round(pre_list[i]))\n",
        "  ans_name_list.append(ans_name)\n",
        "# print(ans_list)\n",
        "ans_trans = []\n",
        "for i, ans_val in enumerate(ans_list):\n",
        "  if ans_val == 1:\n",
        "    ans_trans.append(ans_name_list[i] + \"_wrong\")\n",
        "  else:\n",
        "    ans_trans.append(ans_name_list[i] + \"_correct\")\n",
        "print(ans_trans)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}